import streamlit as st
import psycopg2
import numpy as np
import regex as re
from sentence_transformers import SentenceTransformer
from collections import Counter
from pgvector.psycopg2 import register_vector
import html
import time
from datetime import datetime
from bs4 import BeautifulSoup
import markdown

from dotenv import load_dotenv
import os

load_dotenv()  # Load variables from .env file

PASSWORD = os.getenv("DB_PASSWORD")
USER = os.getenv("DB_USER")
HOST = os.getenv("DB_HOST")
PORT = os.getenv("DB_PORT")
DB_NAME = os.getenv("DB_NAME")

SUPABASE_DB_URL = f"postgresql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB_NAME}"

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ”¿ç­–æ–‡æ¡£æ™ºèƒ½æœç´¢ç³»ç»Ÿ",
    page_icon="ğŸ“˜",
    layout="wide",
    initial_sidebar_state="expanded",
)


# åˆå§‹åŒ–å‘é‡æ¨¡å‹
@st.cache_resource
def load_model():
    return SentenceTransformer("moka-ai/m3e-base")


model = load_model()


# è¿æ¥ Supabase
def get_db_connection():
    try:
        conn = psycopg2.connect(SUPABASE_DB_URL)
        register_vector(conn)  # æ³¨å†Œ `pgvector`
        return conn
    except Exception as e:
        st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        return None


# ä¾§è¾¹æ ç­›é€‰å™¨
# st.sidebar.title("æœç´¢è®¾ç½®")


# ç¼“å­˜ç­›é€‰é€‰é¡¹
@st.cache_data(ttl=3600)  # 1å°æ—¶ç¼“å­˜
def load_filters():
    """åŠ è½½å¯ç”¨çš„åˆ†ç±»ç­›é€‰æ•°æ®"""
    conn = get_db_connection()
    if not conn:
        return [], [], []

    cursor = conn.cursor()

    cursor.execute("SELECT DISTINCT location FROM test_documents")
    locations = [row[0] for row in cursor.fetchall() if row[0]]

    cursor.execute("SELECT DISTINCT category FROM test_documents")
    categories = [row[0] for row in cursor.fetchall() if row[0]]

    cursor.execute("SELECT DISTINCT publish_date FROM test_documents")
    dates = [str(row[0]) for row in cursor.fetchall() if row[0]]

    conn.close()
    return locations, categories, dates


# ç¼“å­˜æ–‡ä»¶åˆ—è¡¨
@st.cache_data(ttl=3600)  # 1å°æ—¶ç¼“å­˜
def load_files():
    """åŠ è½½æ–‡ä»¶åˆ—è¡¨"""
    conn = get_db_connection()
    if not conn:
        return []

    cursor = conn.cursor()
    cursor.execute(
        """
        SELECT DISTINCT file_name, category, location, publish_date 
        FROM test_documents 
        ORDER BY file_name
    """
    )
    files = [(row[0], row[1], row[2], row[3]) for row in cursor.fetchall()]
    conn.close()
    return files


# é€šç”¨å·¥å…·å‡½æ•°
def contains_number(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åŒ…å«æ•°å­—/å¹´ä»½/é‡‘é¢/ç™¾åˆ†æ¯”"""
    number_pattern = r"(\d{4}å¹´|\$\d+(?:,\d+)*|\d+\.\d+%|\d+)"
    return bool(re.search(number_pattern, text))


def highlight_keywords(text, query):
    """é«˜äº®å…³é”®è¯"""
    query = html.escape(query)
    text = html.escape(text)

    keywords = query.split()
    for word in keywords:
        if word.strip():
            pattern = rf"({re.escape(word)})"
            text = re.sub(
                pattern,
                r'<span style="color:red; font-weight:bold;">\1</span>',
                text,
                flags=re.IGNORECASE,
            )
    return text


# è·å–ä¸Šä¸‹æ–‡
def get_context(segment_id, max_context=3):
    """è·å–æ®µè½çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå‡ ä¸ªæ®µè½ï¼‰"""
    conn = get_db_connection()
    if not conn:
        return None

    cursor = conn.cursor()

    # è·å–å½“å‰æ®µè½æ‰€å±æ–‡æ¡£ID
    cursor.execute(
        """
        SELECT document_id FROM test_document_relations WHERE segment_id = %s
    """,
        (segment_id,),
    )
    row = cursor.fetchone()
    if not row:
        conn.close()
        return None

    document_id = row[0]

    # è·å–å½“å‰æ®µè½çš„åºå·
    cursor.execute(
        """
        SELECT segment_position FROM test_document_relations 
        WHERE document_id = %s AND segment_id = %s
    """,
        (document_id, segment_id),
    )
    position_row = cursor.fetchone()
    current_position = position_row[0] if position_row else 0

    # è·å–ä¸Šä¸‹æ–‡æ®µè½
    cursor.execute(
        """
        SELECT r.segment_position, s.id, s.segment_text 
        FROM test_document_relations r
        JOIN test_document_segments s ON r.segment_id = s.id
        WHERE r.document_id = %s 
        AND r.segment_position BETWEEN %s AND %s
        ORDER BY r.segment_position
    """,
        (
            document_id,
            max(0, current_position - max_context),
            current_position + max_context,
        ),
    )

    context = []
    for pos, seg_id, text in cursor.fetchall():
        context.append(
            {
                "position": pos,
                "segment_id": seg_id,
                "text": text,
                "is_current": seg_id == segment_id,
            }
        )

    conn.close()
    return context


def search(query, selected_files, selected_location, selected_category, selected_date):
    """åœ¨ `pgvector` è¿›è¡Œå‘é‡æœç´¢ï¼Œæ”¯æŒå¤šç»´åº¦è¿‡æ»¤å’Œè¯­ä¹‰æœç´¢"""
    # å¤„ç†ç©ºæŸ¥è¯¢çš„æƒ…å†µ
    if not query.strip():
        return []

    # åˆ†ææŸ¥è¯¢å…³é”®è¯
    keywords = query.split()

    # åˆ›å»ºæŸ¥è¯¢å‘é‡
    query_vector = model.encode([query], convert_to_numpy=True)[0]

    conn = get_db_connection()
    cursor = conn.cursor()

    conditions = []
    params = [query_vector]

    if selected_files and selected_files != all_files:
        conditions.append(
            f"test_documents.file_name IN ({', '.join(['%s'] * len(selected_files))})"
        )
        params.extend(selected_files)

    if selected_location and selected_location != locations:
        placeholders = ", ".join(["%s"] * len(selected_location))
        conditions.append(f"test_documents.location IN ({placeholders})")
        params.extend(selected_location)

    if selected_category:
        conditions.append("test_documents.category = %s")
        params.append(selected_category)

    if selected_date:
        conditions.append("test_documents.publish_date = %s")
        params.append(selected_date)

    where_clause = " AND ".join(conditions) if conditions else "1=1"

    # ä¼˜åŒ–æŸ¥è¯¢ä»¥åŒ…å«æ›´å¤šå…ƒæ•°æ®å’Œç›¸å…³æ€§ä¿¡æ¯
    sql_query = f"""
        SELECT 
            test_documents.file_name, 
            test_documents.category,
            test_documents.location,
            test_documents.publish_date,
            test_document_segments.id, 
            test_document_segments.segment_text,
            test_document_segments.vector_embedding <=> %s AS distance
        FROM test_document_relations
        JOIN test_documents ON test_document_relations.document_id = test_documents.id
        JOIN test_document_segments ON test_document_relations.segment_id = test_document_segments.id
        WHERE {where_clause}
        ORDER BY distance ASC
        LIMIT 100;  -- å¢åŠ åˆå§‹è·å–é‡ä»¥æ”¹å–„å¤šæ ·æ€§
    """

    cursor.execute(sql_query, params)
    results = cursor.fetchall()

    doc_scores = {}
    doc_texts = {}
    doc_metadata = {}

    # æ›´å¤æ‚çš„ç›¸å…³æ€§è®¡ç®—
    for row in results:
        (
            file_name,
            category,
            location,
            publish_date,
            segment_id,
            text_segment,
            distance,
        ) = row

        # æ”¹è¿›çš„ç›¸å…³æ€§è¯„åˆ†
        base_score = 1 / (1 + distance)  # å‘é‡åŒ¹é…å¾—åˆ†

        # åŠ æƒå› å­
        keyword_match_bonus = 1.0
        for keyword in keywords:
            if re.search(rf"\b{re.escape(keyword)}\b", text_segment, re.IGNORECASE):
                keyword_match_bonus += 0.2  # å¢åŠ å…³é”®è¯åŒ¹é…æƒé‡

        number_bonus = 1.2 if contains_number(text_segment) else 1.0  # æ•°å€¼åŠ æƒ
        length_penalty = min(
            1.0, max(0.8, len(text_segment) / 500)
        )  # é•¿åº¦æƒ©ç½šï¼Œé¿å…è¿‡é•¿æ–‡æœ¬

        # ç»¼åˆè¯„åˆ†
        final_score = base_score * number_bonus * keyword_match_bonus * length_penalty

        if file_name not in doc_scores:
            doc_scores[file_name] = 0
            doc_texts[file_name] = []
            doc_metadata[file_name] = {
                "category": category,
                "location": location,
                "publish_date": publish_date,
            }

        doc_scores[file_name] += final_score
        doc_texts[file_name].append((segment_id, text_segment, final_score))

    cursor.close()
    conn.close()

    # é€‰å‡ºæ’åå‰ 10 çš„æœ€ä½³æ–‡æ¡£
    top_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:10]

    final_results = []
    for file_name, _ in top_docs:
        # è·å–æ–‡æ¡£çš„å…ƒæ•°æ®
        metadata = doc_metadata[file_name]

        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½ï¼Œä½†ç¡®ä¿å®ƒä»¬ä¸æ˜¯é‡å¤å†…å®¹
        paragraphs = doc_texts[file_name]
        paragraphs.sort(key=lambda x: x[2], reverse=True)

        # å»é‡ï¼šç¡®ä¿ä¸é€‰æ‹©å†…å®¹è¿‡äºç›¸ä¼¼çš„æ®µè½
        unique_paragraphs = []
        used_content = set()

        for segment_id, text, score in paragraphs:
            # ç®€å•çš„å»é‡æœºåˆ¶ï¼šæ£€æŸ¥å‰50ä¸ªå­—ç¬¦æ˜¯å¦é‡å¤
            text_start = text[:50].lower()
            if text_start not in used_content and len(unique_paragraphs) < 3:
                unique_paragraphs.append((segment_id, text, score))
                used_content.add(text_start)

        final_results.append(
            {
                "file_name": file_name,
                "category": metadata["category"],
                "location": metadata["location"],
                "publish_date": metadata["publish_date"],
                "best_paragraphs": unique_paragraphs,
            }
        )

    return final_results


import html


def clean_to_plain_text_with_titles(text, query=None):
    # 1. Remove LaTeX syntax ($...$)
    text = re.sub(r"\$.*?\$", "", text)

    # 2. Replace markdown headers (e.g., `# Title` â†’ `Title`) and preserve titles/subtitles
    text = re.sub(r"^(#{1,6})\s*(.*)", lambda m: f"{m.group(2)}", text)

    # 3. Convert markdown line breaks (like `\n`) to `<br>` to preserve the structure
    text = text.replace("\n", " <br> ")

    # 4. Remove HTML tags (if any)
    soup = BeautifulSoup(text, "html.parser")
    plain_text = soup.get_text(separator=" ")

    # 5. Unescape HTML entities (like `&nbsp;` to spaces)
    plain_text = html.unescape(plain_text)

    # 6. Optional: Highlight query keywords in plain text
    if query:
        for word in query.split():
            if word.strip():
                pattern = re.compile(re.escape(word), re.IGNORECASE)
                plain_text = pattern.sub(lambda m: f"**{m.group(0)}**", plain_text)

    return plain_text.strip()


# æ˜¾ç¤ºæœç´¢ç»“æœ
def display_search_results(query, results):
    # ç»“æœé¢æ¿
    st.subheader("ğŸ“ æœç´¢ç»“æœ")

    # åˆ†ç±»æ±‡æ€»
    categories = Counter([doc["category"] for doc in results if doc["category"]])
    locations = Counter([doc["location"] for doc in results if doc["location"]])

    # åœ¨æœç´¢ç»“æœä¹‹ä¸Šæ˜¾ç¤ºæœç´¢æ‘˜è¦
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**æŒ‰ç±»åˆ«åˆ†å¸ƒ:**")
        for cat, count in categories.most_common():
            st.markdown(f"- {cat}: {count}ä¸ªæ–‡æ¡£")

    with col2:
        st.markdown("**æŒ‰åœ°åŒºåˆ†å¸ƒ:**")
        for loc, count in locations.most_common():
            st.markdown(f"- {loc}: {count}ä¸ªæ–‡æ¡£")

    # æ˜¾ç¤ºç»“æœåˆ—è¡¨
    for idx, doc in enumerate(results):
        file_name = doc["file_name"]
        category = doc["category"] if doc["category"] else "æœªåˆ†ç±»"
        location = doc["location"] if doc["location"] else "æœªçŸ¥åœ°åŒº"
        publish_date = doc["publish_date"] if doc["publish_date"] else "æœªçŸ¥æ—¥æœŸ"

        with st.expander(
            f"ğŸ“„ {file_name} [{category} - {location} - {publish_date}]",
            expanded=idx == 0,
        ):
            # ä¸ºæ¯ä¸ªæœç´¢ç»“æœæ·»åŠ ç›¸å…³åº¦ä¿¡æ¯
            st.markdown("##### æœ€ç›¸å…³æ®µè½")

            # æ˜¾ç¤ºæ®µè½
            for i, (segment_id, paragraph, score) in enumerate(doc["best_paragraphs"]):
                # Clean and render as plain text
                plain_text = clean_to_plain_text_with_titles(paragraph, query)

                # Display with a simple card style
                st.markdown(
                    f"""
                <div style='background-color:{"#ffebcc" if i==0 else "#f9f9f9"}; 
                            padding:15px; 
                            border-radius:8px; 
                            margin-bottom:10px;
                            border-left:4px solid {"#ff9900" if i==0 else "#cccccc"};'>
                    <div style='display:flex; justify-content:space-between; margin-bottom:5px;'>
                        <span style='font-weight:bold;'>{"ğŸ”¥ æœ€ä½³åŒ¹é…" if i==0 else f"åŒ¹é… #{i+1}"}</span>
                        <span style='color:#666;'>ç›¸å…³åº¦: {score:.2f}</span>
                    </div>
                """,
                    unsafe_allow_html=True,
                )

                # Show the cleaned text
                st.markdown(f"> {plain_text}")

                st.markdown("</div>", unsafe_allow_html=True)

                # if "context_visible" not in st.session_state:
                #     st.session_state.context_visible = {}

                # # æ›¿æ¢æŒ‰é’®å¤„ç†é€»è¾‘
                # context_key = f"context_{segment_id}"
                # if st.button(f"ğŸ“‘ æŸ¥çœ‹ä¸Šä¸‹æ–‡", key=f"btn_{context_key}"):
                #     # åˆ‡æ¢æ˜¾ç¤ºçŠ¶æ€
                #     st.session_state.context_visible[context_key] = not st.session_state.context_visible.get(context_key, False)

                # # æ ¹æ®çŠ¶æ€æ˜¾ç¤ºä¸Šä¸‹æ–‡
                # if st.session_state.context_visible.get(context_key, False):
                #     context_paragraphs = get_context(segment_id)
                #     if context_paragraphs:
                #         st.markdown("##### ä¸Šä¸‹æ–‡æ®µè½")
                #         for ctx in context_paragraphs:
                #             is_current = ctx["is_current"]
                #             ctx_text = highlight_keywords(ctx["text"], query) if is_current else ctx["text"]

                #             st.markdown(f"""
                #             <div style='background-color:{"#e6f7ff" if is_current else "#ffffff"};
                #                        padding:10px;
                #                        border-radius:5px;
                #                        margin-bottom:5px;
                #                        border-left:3px solid {"#1890ff" if is_current else "#e8e8e8"};'>
                #                 {ctx_text}
                #             </div>
                #             """, unsafe_allow_html=True)
                #     else:
                #         st.info("æ— æ³•è·å–ä¸Šä¸‹æ–‡å†…å®¹")

                # æä¾›ä¸Šä¸‹æ–‡æŸ¥çœ‹æŒ‰é’®
                # if st.button(f"ğŸ“‘ æŸ¥çœ‹ä¸Šä¸‹æ–‡", key=f"context_{segment_id}"):
                #     context_paragraphs = get_context(segment_id)
                #     if context_paragraphs:
                #         st.markdown("##### ä¸Šä¸‹æ–‡æ®µè½")
                #         for ctx in context_paragraphs:
                #             is_current = ctx["is_current"]
                #             ctx_text = highlight_keywords(ctx["text"], query) if is_current else ctx["text"]

                #             st.markdown(f"""
                #             <div style='background-color:{"#e6f7ff" if is_current else "#ffffff"};
                #                        padding:10px;
                #                        border-radius:5px;
                #                        margin-bottom:5px;
                #                        border-left:3px solid {"#1890ff" if is_current else "#e8e8e8"};'>
                #                 {ctx_text}
                #             </div>
                #             """, unsafe_allow_html=True)
                #     else:
                #         st.info("æ— æ³•è·å–ä¸Šä¸‹æ–‡å†…å®¹")

            # æ·»åŠ å¯¼å‡ºé€‰é¡¹
            # col1, col2 = st.columns(2)
            # with col1:
            #     if st.button("ğŸ“‹ å¤åˆ¶åˆ°å‰ªè´´æ¿", key=f"copy_{file_name}"):
            #         extract_text = "\n\n".join([para for _, para, _ in doc["best_paragraphs"]])
            #         st.code(extract_text)
            #         st.success("å†…å®¹å·²å¤åˆ¶ï¼Œè¯·ä½¿ç”¨Ctrl+Cå¤åˆ¶ä¸Šæ–¹ä»£ç æ¡†ä¸­çš„æ–‡æœ¬")

            # with col2:
            #     # æ·»åŠ æŸ¥çœ‹åŸæ–‡é€‰é¡¹
            #     st.markdown(f"[ğŸ“– æŸ¥çœ‹å®Œæ•´æ–‡æ¡£](#{file_name})")


# æ·»åŠ æ•°æ®åé¦ˆåŠŸèƒ½
def feedback_handler():
    if "feedback_given" not in st.session_state:
        st.session_state.feedback_given = set()

    def give_feedback(segment_id, rating):
        if segment_id in st.session_state.feedback_given:
            return

        conn = get_db_connection()
        if not conn:
            st.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“æäº¤åé¦ˆ")
            return

        cursor = conn.cursor()

        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰åé¦ˆè¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS user_feedback (
                    id SERIAL PRIMARY KEY,
                    segment_id INTEGER NOT NULL,
                    rating INTEGER NOT NULL, -- 1-5åˆ†
                    feedback_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )
            conn.commit()

            # è®°å½•åé¦ˆ
            cursor.execute(
                """
                INSERT INTO user_feedback (segment_id, rating)
                VALUES (%s, %s)
            """,
                (segment_id, rating),
            )
            conn.commit()

            st.session_state.feedback_given.add(segment_id)
            st.success("è°¢è°¢æ‚¨çš„åé¦ˆï¼")
        except Exception as e:
            st.error(f"æäº¤åé¦ˆå¤±è´¥: {str(e)}")
        finally:
            conn.close()

    return give_feedback


# æ·»åŠ å¯¼å‡ºæ–‡æ¡£åŠŸèƒ½
def generate_report(query, results):
    """ç”Ÿæˆå¯å¯¼å‡ºçš„æŠ¥å‘Š"""
    report = f"""# æœç´¢æŠ¥å‘Šï¼š"{query}"
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## æœç´¢ç»“æœæ‘˜è¦
å…±æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£

"""
    # æ·»åŠ æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹
    for i, doc in enumerate(results):
        report += f"""
### {i+1}. {doc['file_name']}
- åˆ†ç±»: {doc['category'] if doc['category'] else 'æœªåˆ†ç±»'}
- åœ°åŒº: {doc['location'] if doc['location'] else 'æœªçŸ¥'}
- å‘å¸ƒæ—¥æœŸ: {doc['publish_date'] if doc['publish_date'] else 'æœªçŸ¥'}

#### ç›¸å…³æ®µè½:
"""
        for j, (_, para, score) in enumerate(doc["best_paragraphs"]):
            report += f"""
**æ®µè½ {j+1}** (ç›¸å…³åº¦: {score:.2f})
{para}

"""

    return report


def save_search_history_to_db(query_text, timestamp):
    conn = get_db_connection()
    if not conn:
        return

    cursor = conn.cursor()

    try:
        # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºè¡¨
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS user_queries (
                id SERIAL PRIMARY KEY,
                query_text TEXT NOT NULL,
                query_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
            )
        """
        )
        conn.commit()

        # æ’å…¥æŸ¥è¯¢è®°å½•
        cursor.execute(
            """
            INSERT INTO user_queries (query_text, query_time)
            VALUES (%s, %s)
        """,
            (query_text, timestamp),
        )
        conn.commit()
    except Exception as e:
        st.error(f"æ— æ³•ä¿å­˜æŸ¥è¯¢å†å²: {str(e)}")
    finally:
        conn.close()


# é…ç½®å…¨å±€å‚æ•°
# st.sidebar.markdown("### åº”ç”¨è®¾ç½®")
# # å…è®¸ç”¨æˆ·è°ƒæ•´æœç´¢è®¾ç½®
# max_results = st.sidebar.slider("æœ€å¤§ç»“æœæ•°é‡", 5, 20, 10)
# relevance_threshold = st.sidebar.slider("æœ€ä½ç›¸å…³åº¦é˜ˆå€¼", 0.1, 0.9, 0.5)
max_results = 10
relevance_threshold = 0.5

# è¿è¡Œä¸»åº”ç”¨
# åŠ è½½æ•°æ®
locations, categories, dates = load_filters()
files_data = load_files()
all_files = [f[0] for f in files_data]

# æ ‡é¢˜åŒºåŸŸ
st.title("ğŸ“˜ æ”¿ç­–æ–‡æ¡£æ™ºèƒ½æœç´¢ç³»ç»Ÿï¼ˆå±•ç¤ºç‰ˆï¼‰")

st.write(
    """
    - 2024å¹´12æœˆå…¨å›½å‚¨èƒ½æ”¿ç­– + 136å·æ–‡ä»¶ + 1ä¸ªæ ‡å‡†
    - æ··åˆä½¿ç”¨å…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰æœç´¢ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„æ®µè½
    - å¯¹äºåŒ…å«æ•°å­—ã€å¹´ä»½ã€é‡‘é¢ç­‰çš„æ–‡æœ¬ï¼Œä¼šæœ‰é¢å¤–çš„åŠ æƒä»¥æé«˜ç›¸å…³æ€§
    - **åç»­æœåŠ¡å™¨æ¥å…¥ä¹‹åï¼Œå¯ä»¥æ”¯æŒå…¨æ–‡è®¿é—®**
"""
)

# æœç´¢æ 
col1, col2 = st.columns([3, 1])
with col1:
    query = st.text_input("ğŸ” è¯·è¾“å…¥æœç´¢å…³é”®è¯", key="search_box")
with col2:
    search_button = st.button("å¼€å§‹æœç´¢", use_container_width=True)

# é«˜çº§ç­›é€‰åŒº
with st.expander("é«˜çº§ç­›é€‰é€‰é¡¹", expanded=False):
    col1, col2, col3 = st.columns(3)

    with col1:
        selected_locations = st.multiselect("ğŸŒ é€‰æ‹©åœ°ç‚¹", locations, default=locations)

    with col2:
        selected_category = st.selectbox("ğŸ“‚ é€‰æ‹©ç±»åˆ«", [""] + categories, index=0)

    with col3:
        selected_date = st.selectbox("ğŸ“… é€‰æ‹©æ—¶é—´", [""] + dates, index=0)

# æœç´¢å†å²è®°å½•
if "search_history" not in st.session_state:
    st.session_state.search_history = []

# å¤„ç†æœç´¢è¯·æ±‚
if search_button and query:
    start_time = time.time()

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # âœ… ä¿å­˜å†å²åˆ°æ•°æ®åº“
    save_search_history_to_db(query, timestamp)

    with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³å†…å®¹..."):
        best_match_docs = search(
            query, all_files, selected_locations, selected_category, selected_date
        )

        if best_match_docs:
            end_time = time.time()
            st.success(
                f"âœ… æ‰¾åˆ° {len(best_match_docs)} ä¸ªç›¸å…³æ–‡æ¡£ (ç”¨æ—¶ {end_time - start_time:.2f} ç§’)"
            )
            display_search_results(query, best_match_docs)
        else:
            st.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·å°è¯•ä¸åŒçš„å…³é”®è¯ã€‚")


# if search_button and query:
#     start_time = time.time()

#     # æ·»åŠ åˆ°æœç´¢å†å²
#     timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#     st.session_state.search_history.insert(0, {
#         "query": query,
#         "timestamp": timestamp
#     })

#     # æœ€å¤šä¿ç•™10æ¡å†å²è®°å½•
#     if len(st.session_state.search_history) > 10:
#         st.session_state.search_history = st.session_state.search_history[:10]

#     with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³å†…å®¹..."):
#         best_match_docs = search(query, all_files, selected_locations, selected_category, selected_date)

#         # æ˜¾ç¤ºæœç´¢ç»“æœ
#         if best_match_docs:
#             end_time = time.time()
#             st.success(f"âœ… æ‰¾åˆ° {len(best_match_docs)} ä¸ªç›¸å…³æ–‡æ¡£ (ç”¨æ—¶ {end_time - start_time:.2f} ç§’)")

#             # æ˜¾ç¤ºç»“æœ
#             display_search_results(query, best_match_docs)
#         else:
#             st.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·å°è¯•ä¸åŒçš„å…³é”®è¯ã€‚")
# æ˜¾ç¤ºæœç´¢å†å²
# if st.session_state.search_history:
#     st.sidebar.subheader("æœç´¢å†å²")
#     for idx, item in enumerate(st.session_state.search_history):
#         if st.sidebar.button(f"{item['query']} ({item['timestamp']})", key=f"history_{idx}"):
#             # ç‚¹å‡»å†å²è®°å½•æ—¶ï¼Œå¡«å……æœç´¢æ¡†å¹¶æ‰§è¡Œæœç´¢
#             st.session_state.search_box = item['query']
#             st.experimental_rerun()


# é¡µè„š
st.markdown("---")
st.markdown("ğŸ“˜ **æ”¿ç­–æ–‡æ¡£æ™ºèƒ½æœç´¢ç³»ç»Ÿ** | åŸºäºè¯­ä¹‰å‘é‡æ£€ç´¢")
